from: https://www.bilibili.com/video/BV1GT4y1A756

# 第1章 k8s概念和架构

## P1-尚硅谷-Kubernetes课程内容介绍

## P2-尚硅谷-Kubernetes介绍和特性

## P3-尚硅谷-Kubernetes集群架构组件

## P4-尚硅谷-Kubernetes核心概念

# 第2章 集群搭建

## P5-尚硅谷-搭建Kubernetes集群（平台规划和部署方式介绍）

## kubeadm方式

### P6-尚硅谷-搭建Kubernetes集群（kubeadm方式）-操作系统初始化
### P7-尚硅谷-搭建Kubernetes集群（kubeadm方式）-部署master节点
### P8-尚硅谷-搭建Kubernetes集群（kubeadm方式）-部署node节点和集群测试

## 二进制方式

### P9-尚硅谷-搭建Kubernetes集群（二进制方式）-搭建步骤介绍
### P10-尚硅谷-搭建Kubernetes集群（二进制方式）-操作系统初始化
### P11-尚硅谷-搭建Kubernetes集群（二进制方式）-生成SSL证书文件
### P12-尚硅谷-搭建Kubernetes集群（二进制方式）-部署etcd集群
### P13-尚硅谷-搭建Kubernetes集群（二进制方式）-为APIServer自签证书
### P14-尚硅谷-搭建Kubernetes集群（二进制方式）-部署Master组件
### P15-尚硅谷-搭建Kubernetes集群（二进制方式）-部署Node组件（安装Docker）
### P16-尚硅谷-搭建Kubernetes集群（二进制方式）-部署Node组件（kubelet和kube-proxy）
### P17-尚硅谷-搭建Kubernetes集群（二进制方式）-部署CNI网络和集群测试

## P18-尚硅谷-搭建Kubernetes集群（两种搭建方式总结）

# 第3章 核心概念

## P20-尚硅谷-Kubernetes核心技术-资源编排（yaml）介绍
## P21-尚硅谷-Kubernetes核心技术-资源编排（yaml）编写方式
## P22-尚硅谷-Kubernetes核心技术-Pod（概述和存在意义）
## P23-尚硅谷-Kubernetes核心技术-Pod（两种实现机制）
## P24-尚硅谷-Kubernetes核心技术-Pod（镜像拉取 重启策略和资源限制）
## P25-尚硅谷-Kubernetes核心技术-Pod（健康检查）
## P26-尚硅谷-Kubernetes核心技术-Pod（调度策略）-创建Pod流程
## P27-尚硅谷-Kubernetes核心技术-Pod（调度策略）-影响Pod调度（资源限制和节点选择器）
## P28-尚硅谷-Kubernetes核心技术-Pod（调度策略）-影响Pod调度（节点亲和性）
## P29-尚硅谷-Kubernetes核心技术-Pod（调度策略）-影响Pod调度（污点和污点容忍）
## P30-尚硅谷-Kubernetes核心技术-Controller（Deployment）-概述和应用场景
## P31-尚硅谷-Kubernetes核心技术-Controller（Deployment）-发布应用
## P32-尚硅谷-Kubernetes核心技术-Controller（Deployment）-升级回滚和弹性伸缩
## P33-尚硅谷-Kubernetes核心技术-Service-概述
## P34-尚硅谷-Kubernetes核心技术-Service-三种类型
## P35-尚硅谷-Kubernetes核心技术-Controller（StatefulSet）-部署有状态应用
## P36-尚硅谷- Kubernetes核心技术-Controller（DaemonSet）-部署守护进程
## P37-尚硅谷-Kubernetes核心技术-Controller（Job和Cronjob）-一次任务和定时任务
## P38-尚硅谷-Kubernetes核心技术-配置管理-Secret
## P39-尚硅谷-Kubernetes核心技术-配置管理-ConfigMap
## P40-尚硅谷-Kubernetes核心技术-集群安全机制（概述）
## P41-尚硅谷-Kubernetes核心技术-集群安全机制（RBAC介绍）
## P42-尚硅谷-Kubernetes核心技术-集群安全机制（RBAC实现鉴权）
## P43-尚硅谷-Kubernetes核心技术-Ingress（概述）
## P44-尚硅谷-Kubernetes核心技术-Ingress（对外暴露应用）
## P45-尚硅谷-Kubernetes核心技术-Helm（引入）
## P46-尚硅谷-Kubernetes核心技术-Helm（概述）
## P47-尚硅谷-Kubernetes核心技术-Helm（安装和配置仓库）
## P48-尚硅谷-Kubernetes核心技术-Helm（快速部署应用）
## P49-尚硅谷-Kubernetes核心技术-Helm（自定义chart部署）
## P50-尚硅谷-Kubernetes核心技术-Helm（chart模板使用）
## P51-尚硅谷-Kubernetes核心技术-Helm（chart模板使用）
## P52-尚硅谷-Kubernetes核心技术-持久化存储（nfs网络存储）
## P53-尚硅谷-Kubernetes核心技术-持久化存储（pv和pvc）

# 第4章 集群监控平台

## P54-尚硅谷-Kubernetes集群资源监控-监控指标和方案

### 1、监控指标

- 集群监控
  - 节点资源利用率
  - 节点数
  - 运行的pods
- pod监控
  - 容器指标
  - 应用程序

### 2、监控平台 prometheus+grafana

- prometheus
  - 开源的、报警、数据库。
  - 以http协议周期性抓取被监控组件。
  - 不需要复杂的集成过程，使用http接口接入就可以了
- Grafana
  - 开源的，数据分析和可视化工具
  - 支持多种数据据源

## P55-尚硅谷-Kubernetes集群资源监控-搭建监控平台

### 1、部署prometheus

```yaml
rbac-setup.yaml 	# 配置角色，权限
configmap.yaml		# 配置参数
prometheus.deploy.yml	# deploy
prometheus.svc.yml		# service
node-exporter.yaml		# DaemonSet 守护进程
```

先部署守护进程node-exporter.yaml

```yaml
---
apiVersion: apps/v1	# 正式用apps/v1   测试 extensions/v1beta1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: kube-system
  labels:
    k8s-app: node-exporter
spec:
  selector: 
    matchLabels:
      k8s-app: node-exporter
  template:
    metadata:
      labels:
        k8s-app: node-exporter
    spec:
      containers:
      - image: prom/node-exporter
        name: node-exporter
        ports:
        - containerPort: 9100
          protocol: TCP
          name: http
---
apiVersion: v1
kind: Service
metadata:
  labels:
    k8s-app: node-exporter
  name: node-exporter
  namespace: kube-system
spec:
  ports:
  - name: http
    port: 9100
    nodePort: 31672
    protocol: TCP
  type: NodePort
  selector:
    k8s-app: node-exporter
```

2、rbac-setup.yaml

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [""]
  resources:
  - nodes
  - nodes/proxy
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups:
  - extensions
  resources:
  - ingresses
  verbs: ["get", "list", "watch"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: kube-system
```

3、configmap.yaml

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: kube-system
data:
  prometheus.yml: |
    global:
      scrape_interval:     15s
      evaluation_interval: 15s
    scrape_configs:

    - job_name: 'kubernetes-apiservers'
      kubernetes_sd_configs:
      - role: endpoints
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
        action: keep
        regex: default;kubernetes;https

    - job_name: 'kubernetes-nodes'
      kubernetes_sd_configs:
      - role: node
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics

    - job_name: 'kubernetes-cadvisor'
      kubernetes_sd_configs:
      - role: node
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor

    - job_name: 'kubernetes-service-endpoints'
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
        action: replace
        target_label: __scheme__
        regex: (https?)
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
        action: replace
        target_label: __address__
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_service_name]
        action: replace
        target_label: kubernetes_name

    - job_name: 'kubernetes-services'
      kubernetes_sd_configs:
      - role: service
      metrics_path: /probe
      params:
        module: [http_2xx]
      relabel_configs:
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
        action: keep
        regex: true
      - source_labels: [__address__]
        target_label: __param_target
      - target_label: __address__
        replacement: blackbox-exporter.example.com:9115
      - source_labels: [__param_target]
        target_label: instance
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_service_name]
        target_label: kubernetes_name

    - job_name: 'kubernetes-ingresses'
      kubernetes_sd_configs:
      - role: ingress
      relabel_configs:
      - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path]
        regex: (.+);(.+);(.+)
        replacement: ${1}://${2}${3}
        target_label: __param_target
      - target_label: __address__
        replacement: blackbox-exporter.example.com:9115
      - source_labels: [__param_target]
        target_label: instance
      - action: labelmap
        regex: __meta_kubernetes_ingress_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_ingress_name]
        target_label: kubernetes_name

    - job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: kubernetes_pod_name

```

4、prometheus.deploy.yaml

```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    name: prometheus-deployment
  name: prometheus
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      containers:
      - image: prom/prometheus:v2.0.0
        name: prometheus
        command:
        - "/bin/prometheus"
        args:
        - "--config.file=/etc/prometheus/prometheus.yml"
        - "--storage.tsdb.path=/prometheus"
        - "--storage.tsdb.retention=24h"
        ports:
        - containerPort: 9090
          protocol: TCP
        volumeMounts:
        - mountPath: "/prometheus"
          name: data
        - mountPath: "/etc/prometheus"
          name: config-volume
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
          limits:
            cpu: 500m
            memory: 2500Mi
      serviceAccountName: prometheus    
      volumes:
      - name: data
        emptyDir: {}
      - name: config-volume
        configMap:
          name: prometheus-config   

```

5、prometheus.svc.yaml

```yaml
---
kind: Service
apiVersion: v1
metadata:
  labels:
    app: prometheus
  name: prometheus
  namespace: kube-system
spec:
  type: NodePort
  ports:
  - port: 9090
    targetPort: 9090
    nodePort: 30003
  selector:
    app: prometheus
```

```sh
# 检查
kubectl get pod -n kube-system
prometheus-759d85775b-kkf8f       1/1     Running   0          2m25s

```

### 2、Granfana的部署

```yaml
grafana-deploy.yaml	
grafana-ing.yaml	# Ingress
grafana-svc.yaml	# svc
```

grafana-deploy.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana-core
  namespace: kube-system
  labels:
    app: grafana
    component: core
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
      component: core
  template:
    metadata:
      labels:
        app: grafana
        component: core
    spec:
      containers:
      - image: grafana/grafana:4.2.0
        name: grafana-core
        imagePullPolicy: IfNotPresent
        # env:
        resources:
          # keep request = limit to keep this container in guaranteed class
          limits:
            cpu: 100m
            memory: 100Mi
          requests:
            cpu: 100m
            memory: 100Mi
        env:
          # The following env variables set up basic auth twith the default admin user and admin password.
          - name: GF_AUTH_BASIC_ENABLED
            value: "true"
          - name: GF_AUTH_ANONYMOUS_ENABLED
            value: "false"
          # - name: GF_AUTH_ANONYMOUS_ORG_ROLE
          #   value: Admin
          # does not really work, because of template variables in exported dashboards:
          # - name: GF_DASHBOARDS_JSON_ENABLED
          #   value: "true"
        readinessProbe:
          httpGet:
            path: /login
            port: 3000
          # initialDelaySeconds: 30
          # timeoutSeconds: 1
        volumeMounts:
        - name: grafana-persistent-storage
          mountPath: /var
      volumes:
      - name: grafana-persistent-storage
        emptyDir: {}

```


grafana-svc.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: kube-system
  labels:
    app: grafana
    component: core
spec:
  type: NodePort
  ports:
    - port: 3000
  selector:
    app: grafana
    component: core
```
grafana-ing.yaml

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
   name: grafana
   namespace: kube-system
spec:
   rules:
   - host: k8s.grafana
     http:
       paths:
       - path: /
         backend:
          serviceName: grafana
          servicePort: 3000
```

```sh
# 检查
kubectl get pod -n kube-system
grafana-core-7789756d87-kt9jp     1/1     Running   0          2m41s
kube-apiserver-m1                 1/1     Running   3          35d
kube-controller-manager-m1        1/1     Running   3          35d
kube-flannel-ds-2jw9l             1/1     Running   0          23d
kube-flannel-ds-4pl88             1/1     Running   0          35d
kube-flannel-ds-9xlvq             1/1     Running   3          35d
kube-proxy-g55m6                  1/1     Running   0          16d
kube-proxy-jf4sm                  1/1     Running   0          16d
kube-proxy-npw2w                  1/1     Running   1          16d
kube-scheduler-m1                 1/1     Running   3          35d
metrics-server-6b976979db-tj5hs   1/1     Running   0          24h
node-exporter-j2vff               1/1     Running   0          22m
node-exporter-sbpg6               1/1     Running   0          22m
prometheus-759d85775b-kkf8f       1/1     Running   0          15m

```

### 3、打开Grafana，配置数据 源，导入显示模板

```sh
kubectl get svc -n kube-system
NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE     SELECTOR
grafana          NodePort    10.97.191.150   <none>        3000:30549/TCP           3m37s   app=grafana,component=core
kube-dns         ClusterIP   10.96.0.10      <none>        53/UDP,53/TCP,9153/TCP   35d     k8s-app=kube-dns
metrics-server   ClusterIP   10.99.241.132   <none>        443/TCP                  18d     k8s-app=metrics-server
node-exporter    NodePort    10.105.20.82    <none>        9100:31672/TCP           25m     k8s-app=node-exporter
prometheus       NodePort    10.98.180.106   <none>        9090:30003/TCP           17m     app=prometheus
```

进入192.168.132.33.30549

- 默认的用户名和密码都是admin
- 配置数据源，使用prometheus
  - 其中http setting 中url 为prometheus的clusster-ip  http://10.98.180.106:9090
  - Dashboard -> import  导入模板 （如果自动下载不到，去官网https://grafana.com/grafana/dashboards/315）

# 第5章 搭建高可用k8s集群

## P56-尚硅谷-Kubernetes集群搭建-搭建高可用集群（实现过程介绍）
## P57-尚硅谷-Kubernetes集群搭建-搭建高可用集群（初始化和部署keepalived）
## P58-尚硅谷-Kubernetes集群搭建-搭建高可用集群（部署haproxy和安装docker及其他组件）
## P59-尚硅谷-Kubernetes集群搭建-搭建高可用集群（部署master1节点初始化）
## P60-尚硅谷-Kubernetes集群搭建-搭建高可用集群（部署master2和node节点）

# 第6章 集群环境中部署项目

## P61-尚硅谷-Kubernetes集群部署项目-容器交付流程介绍

### 容器交付流程

- 开发代码阶段：
  - 编写代码
  - 测试
  - 编写DockerfileJava部署项目流程

- 持续交付/集成：
  - 代码编译打包
  - 制作镜像
  - 上传镜像仓库

- 部署
  - 环境准备
  - pod
  - Service
  - Ingress
- 运维
  - 监控
  - 故障排查
  - 升级优化

### k8s部署java项目（细节过程）

制作镜像-> 上传到镜像仓库 -> 控制器部署镜像 -> 对外暴露应用（Service, Ingress) -> 运维

## P62-尚硅谷-Kubernetes集群部署项目-部署Java项目（制作镜像）

1. 打成jar/war包
2. 做成镜像

Dockerfile

```dockerfile
FROM openjdk:8-jdk-alpine
VOLUME /tmp
ADD ./target/demojenkins.jar demojenkins.jar
ENTRYPOINT ["java","-jar","/demojenkins.jar", "&"]
```

```sh
# 在jar包路径下制作镜像
docker build -t java-demo-01:1.0 . 
# 检查image
docker images
# 运行docker
docker run -d -p 8111:8111 java-demo-01:1.0 -t
# 浏览器访问
ipaddress:8111/user
```

## P63-尚硅谷-Kubernetes集群部署项目-部署Java项目（推送镜像）

3. 上传到镜像服务器（阿里云）

```sh
# 1. 登录阿里云Docker Registry
$ sudo docker login --username=cather****@163.com registry.cn-shanghai.aliyuncs.com

# 用于登录的用户名为阿里云账号全名，密码为开通服务时设置的密码。

#您可以在访问凭证页面修改凭证密码。
#2. 从Registry中拉取镜像
$ sudo docker pull registry.cn-shanghai.aliyuncs.com/jstone01/javademo1:[镜像版本号]

#3. 将镜像推送到Registry
$ sudo docker login --username=cather****@163.com registry.cn-shanghai.aliyuncs.com
$ sudo docker tag [ImageId] registry.cn-shanghai.aliyuncs.com/jstone01/javademo1:[镜像版本号]
# 例
sudo docker tag [ImageId] registry.cn-shanghai.aliyuncs.com/jstone01/javademo1:1.0.1

$ sudo docker push registry.cn-shanghai.aliyuncs.com/jstone01/javademo1:[镜像版本号]
# 例
sudo docker push registry.cn-shanghai.aliyuncs.com/jstone01/javademo1:1.0.1

#请根据实际镜像信息替换示例中的[ImageId]和[镜像版本号]参数。
#4. 选择合适的镜像仓库地址
#从ECS推送镜像时，可以选择使用镜像仓库内网地址。推送速度将得到提升并且将不会损耗您的公网流量。

#如果您使用的机器位于VPC网络，请使用 registry-vpc.cn-shanghai.aliyuncs.com 作为Registry的域名登录。
#5. 示例
#使用"docker tag"命令重命名镜像，并将它通过专有网络地址推送至Registry。
$ sudo docker images
REPOSITORY                                                         TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
registry.aliyuncs.com/acs/agent                                    0.7-dfb6816         37bb9c63c8b2        7 days ago          37.89 MB

$ sudo docker tag 37bb9c63c8b2 registry-vpc.cn-shanghai.aliyuncs.com/acs/agent:0.7-dfb6816

#使用 "docker push" 命令将该镜像推送至远程。
$ sudo docker push registry-vpc.cn-shanghai.aliyuncs.com/acs/agent:0.7-dfb6816
```



## P64-尚硅谷-Kubernetes集群部署项目-部署Java项目（部署镜像暴露应用）

4. 部署

```sh
kubectl create deploy javademo01 --image=registry.cn-shanghai.aliyuncs.com/jstone01/javademo1:1.0.1 --dry-run -o yaml > javademo1.yaml
kubectl apply -f javademo1.yaml
```

5. 暴露应用

```sh
kubectl scale deploy javademo01 --replicas=3	#扩容
kubectl expose deploy javademo01 --port=8111 --target-port=8111 --type=NodePort # 暴露端口
kubectl get svc
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
javademo01   NodePort    10.104.23.232   <none>        8111:31274/TCP   37s

```

